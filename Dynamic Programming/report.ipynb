{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let state X be (b,s), where b is the belief state of the opponent's preference of Rock, Paper, Scissor. And s is the score = number of your wins - number of opponent wins. Let the control state U be your play of Rock, paper, scissor. Let x0 be $(B=[1/3,1/3,1/3], score = 0)$. Let $\\ell(x,u) = 0$. Let q(x) = score. Let the planning horizen T = n, which is the number of games. <br>\n",
    "Since all games are independent of each other, the belief state is sololy dependent on $(S_r^t, S_p^t, S_s^t)$. Let $S_r^t$ = number of rocks the opponent have played at time t. Let $S_p^t$ = number of paper the opponent have played at time t. Let $S_s^t$ = number of scissors the opponent have played at time t. This means that $b_t \\sim Pr(\\bullet|b_{t-1},S_r^t,S_p^t,S_s^t) = Pr(\\bullet|S_r^t,S_p^t,S_s^t)$. Thus, we can simplify the belief state into $((b^t|S_r^t,S_p^t,S_s^t),score)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal policy, $\\Pi^{t}(b^t|S_r^t,S_p^t,S_s^t,score) = u$, would be: At time t, given discrete belief b and current score s, the best control would be $u\\in(rock, paper, scissor)$.<br>\n",
    "Then we calculate the cost and optimal policy. <br>\n",
    "Let Z_t be opponent's move at time t.\n",
    "$$V^{t}(b_t,s_t) = max_{u_t\\in(r,p,s)}(\\ell(b_t,s_t,u_t) + E[V^{t+1}(b_{t+1},s_{t+1}|b_t,s_t,Z_t)] = max_{u_t\\in(r,p,s)}(0 + Pr(Z_t=Rock|b_t)*V^{t+1}[(b_{t+1}|b_t,Z_t=Rock),(s_{t+1}|u_t,s_t,Z_t=Rock)] + Pr(Z_t=Paper|b_t)*V^{t+1}[(b_{t+1}|b_t,Z_t=Paper),(s_{t+1}|u_t,s_t,Z_t=Paper)] + Pr(Z_t=Scissor|b_t)*V^{t+1}[(b_{t+1}|b_t,Z_t=Scissor),(s_{t+1}|u_t,s_t,Z_t=Scissor)] )$$\n",
    "<br>\n",
    "$$\\Pi^{t}(b_t,s_t) = arg\\, max_{u_t\\in(r,p,s)}(\\ell(b_t,s_t,u_t) + E[V^{t+1}(b_{t+1},s_{t+1}|b_t,s_t,Z_t)] = arg\\,max_{u_t\\in(r,p,s)}(0 + Pr(Z_t=Rock|b_t)*V^{t+1}[(b_{t+1}|b_t,Z_t=Rock),(s_{t+1}|u_t,s_t,Z_t=Rock)] + Pr(Z_t=Paper|b_t)*V^{t+1}[(b_{t+1}|b_t,Z_t=Paper),(s_{t+1}|u_t,s_t,Z_t=Paper)] + Pr(Z_t=Scissor|b_t)*V^{t+1}[(b_{t+1}|b_t,Z_t=Scissor),(s_{t+1}|u_t,s_t,Z_t=Scissor)] )$$\n",
    "<br>\n",
    "$Pr(Z_t=Rock|b_t) = 0.5*(b_t\\,of\\,rock) + 0.25*(b_t\\,of\\,paper) + 0.25*(b_t\\,of\\,scissor)$. To calculate $b_{t+1}|b_t,Z_t=Rock$, since $b_t,b_{t+1} \\sim Pr(\\bullet|S_r,S_p,S_s)$, we can simpilfy it into $(b_{t+1}|Z_t=Rock,S_r^t,S_p^t,S_s^t)$. <br>\n",
    "Here is a small example, if T=2, then at 2nd/last game, i.e, terminal stage, the possible scores are $[-2,-1,0,1,2]$. Let $S_r$ = number of rocks the opponent played. Let $S_p$ = number of paper the opponent played. Let $S_s$ = number of scissors the opponent played. The possible belief states would be $b_2|[S_r^2,S_p^2,S_s^2]$ = {$b_2|[2,0,0],b_2|[0,2,0],b_2|[0,0,2],b_2|[1,1,0],b_2|[1,0,1],b_2|[0,1,1]$}, which is equal to {$[1,0,0],[0,1,0],[0,0,1],[0.5,0.5,0],[0.5,0,0.5],[0,0.5,0.5]$}. Then, at 2nd/last game, all possible $(b_t|S_r^t,S_p^t,S_s^t,score)$ pair would be number of possible belief state times number of possible scores $= 6*5 = (4 choose 2)*(2*2+1) = 30$ numbers of possible states at last game. At 1st game, there will be $(3 choose 2)*(2*1+1) = 9$ possible states. At inital state, there is only one state $(D([1/3,1/3,1/3]), 0)$. <br> <br>\n",
    "However, the state grows exponentially. At 100th game, there will be $(102 choose 2)*(2*100+1) = 1,035,351$, which is over a million states at time 100. It will take our laptops too long to solve this. Thus, the solution is to discretize the belief states. Let N = number of discretized belief states. Then, at 100th game, we will only have N*(201) states. Thus, when calculate $b_{t+1}|b_t,Z_t=Rock$, we need to approximate $S_r^t,S_p^t,S_s^t$ given current time and belief state. And when we calculate out $b_{t+1}|Z_t = Rock, S_r^t,S_p^t,S_s^t$, we need to map it into the closest discretized belief state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the follwing plot, the optimal policy has higher means than other two polcies. However, their standard deviation is similar, due to opponent's random plays. The reason why my game score is not very high is that my discrete belief state is relatively low. But, it still takes my laptop around 25 minutes to compute out the optimal policy. If you want to try a different density, change the N_ value in around 7th line in the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/means.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/std.png\" width=\"500\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
